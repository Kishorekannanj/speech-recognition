# -*- coding: utf-8 -*-
"""Copy of OpenAI_Whisper_ASR_Demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JYov2Jg_jxVZbLStzRL2wcjrMrVf-m3v
"""

# Install necessary libraries for audio processing and dataset handling
!pip install librosa  # For audio processing
!pip install datasets  # For handling datasets (HuggingFace)
!pip install soundfile  # For reading .wav files

from google.colab import drive
drive.mount('/content/drive')

# Set your dataset path (edit this to match your folder)
dataset_path = "/content/drive/MyDrive/common_voice"

import librosa
import numpy as np

# Function to preprocess the audio (resample and normalize)
def preprocess_audio(file_path, target_sr=16000):
    # Load the audio file
    audio, sr = librosa.load(file_path, sr=None)
    # Resample the audio to target sampling rate
    audio_resampled = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)
    # Normalize the audio to ensure consistent volume
    audio_normalized = librosa.util.normalize(audio_resampled)
    return audio_normalized

# Example: Process a sample audio file
sample_audio_path =  "/content/batman.mp3"  # Replace with actual audio file path
processed_audio = preprocess_audio(sample_audio_path)

from google.colab import files
uploaded = files.upload()  # Upload your `train.tsv` and a few `.wav` files
dataset_path = "/content"  # Default Colab working directory

import librosa
import numpy as np

def preprocess_audio(file_path, target_sr=16000):
    audio, sr = librosa.load(file_path, sr=None)
    if sr != target_sr:
        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)
    audio = librosa.util.normalize(audio)
    return audio, target_sr

import pandas as pd

# Example data (replace with your actual metrics)
data = {
    "Condition": ["Clean", "Noisy"],
    "Accuracy (%)": [92.5, 78.4],
    "WER (%)": [7.5, 21.6],
    "Latency (ms)": [120, 130]
}

# Create a DataFrame
df_metrics = pd.DataFrame(data)

# Save to CSV
df_metrics.to_csv("speech_kpis.csv", index=False)

import pandas as pd

data = {
    "Condition": ["Clean", "Noisy - Light", "Noisy - Medium", "Noisy - Heavy"],
    "Accuracy (%)": [90.8, 88.5, 82.3, 70.0],
    "WER (%)": [6.2, 11.5, 17.7, 24.0],
    "Latency (ms)": [115, 125, 132, 195]
}

df = pd.DataFrame(data)
df.to_csv("speech_kpis.csv", index=False)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd # Make sure pandas is imported

# Assuming your data is in 'speech_kpis.csv'
df = pd.read_csv("speech_kpis.csv") # Read data from CSV to DataFrame

plt.figure(figsize=(6, 5))
sns.barplot(x='Condition', y='WER (%)', data=df) # Use df here instead of dataset
plt.title('Word Error Rate (WER) under Different Conditions')
plt.ylabel('WER (%)')
plt.xlabel('Audio Condition')
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd # Make sure pandas is imported

# Assuming your data is in 'speech_kpis.csv'
df = pd.read_csv("speech_kpis.csv") # Read data from CSV to DataFrame

plt.figure(figsize=(8, 5))
sns.lineplot(x='Condition', y='Accuracy (%)', marker='o', data=df) # Use df here instead of dataset
plt.title('Accuracy Across Audio Conditions')
plt.ylabel('Accuracy (%)')
plt.xlabel('Condition')
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

# Bar chart of error types
error_types = ['Homophones', 'Accents', 'Noise']
counts = [120, 80, 150]

plt.figure(figsize=(6,4))
sns.barplot(x=error_types, y=counts, palette='pastel')
plt.title("Error Distribution by Type")
plt.ylabel("Number of Errors")
plt.show()

# Simulated WER over 10 epochs
epochs = list(range(1, 11))
wer = [25.0, 21.4, 18.3, 15.9, 14.2, 12.5, 11.7, 10.8, 10.1, 9.6]

plt.figure(figsize=(8,5))
plt.plot(epochs, wer, marker='o', linestyle='-', color='teal')
plt.title("WER Over Training Iterations")
plt.xlabel("Epoch")
plt.ylabel("Word Error Rate (%)")
plt.grid(True)
plt.tight_layout()
plt.show()

dataset_path = "/content/your_audio_folder"  # If your files are stored in this directory in Colab

from google.colab import files
uploaded = files.upload()  # Upload files manually

# Now use the correct folder path
dataset_path = "/content"

import os
os.listdir("/content")

import nltk
from nltk.corpus import wordnet as wn

# Download WordNet if needed
nltk.download('wordnet')

# List of homophones (this is an example, expand it)
homophones = [
    ("sea", "see"),
    ("flower", "flour"),
    ("break", "brake"),
]

# Check for homophones in a sentence
def check_homophones(sentence, homophones):
    words = sentence.split()
    homophone_pairs = []
    for word1, word2 in homophones:
        if word1 in words and word2 in words:
            homophone_pairs.append((word1, word2))
    return homophone_pairs

# Example transcription (replace with actual transcriptions)
transcription = "I see a flower near the sea."

# Check homophones in the transcription
pairs = check_homophones(transcription, homophones)
print("Detected homophones in transcription:", pairs)

!pip install jiwer

from jiwer import wer, cer

# Example data
ground_truth = ["yes", "no", "maybe"]
predictions_clean = ["yes", "no", "maybe"]
predictions_noisy = ["yes", "no", "may be"]

# Evaluate
print("WER (Clean):", wer(ground_truth, predictions_clean))
print("WER (Noisy):", wer(ground_truth, predictions_noisy))

from jiwer import wer, cer

# Ground truth and predictions
ground_truth = ["yes", "no", "maybe"]
predictions_clean = ["yes", "no", "maybe"]
predictions_noisy = ["yes", "no", "may be"]

# WER and CER
print("WER (Clean):", wer(ground_truth, predictions_clean))
print("WER (Noisy):", wer(ground_truth, predictions_noisy))
print("CER (Clean):", cer(ground_truth, predictions_clean))
print("CER (Noisy):", cer(ground_truth, predictions_noisy))

!pip install vosk

!wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip
!unzip -o vosk-model-small-en-us-0.15.zip

from google.colab import files
uploaded = files.upload()

!pip install jiwer

from jiwer import wer, cer
from sklearn.metrics import precision_score, recall_score, f1_score
import time

# Example ground truth and predictions
ground_truth = ["yes", "no", "maybe"]
predicted_clean = ["yes", "no", "maybe"]
predicted_noisy = ["yes", "now", "baby"]

# Calculate WER
print("WER (clean):", wer(ground_truth, predicted_clean))
print("WER (noisy):", wer(ground_truth, predicted_noisy))

# Accuracy
def word_accuracy(gt, pred):
    correct = sum([g == p for g, p in zip(gt, pred)])
    return correct / len(gt)

print("Accuracy (clean):", word_accuracy(ground_truth, predicted_clean))
print("Accuracy (noisy):", word_accuracy(ground_truth, predicted_noisy))

# Precision, Recall, F1 (for multi-class words)
all_labels = list(set(ground_truth + predicted_noisy))
y_true = [all_labels.index(w) for w in ground_truth]
y_pred = [all_labels.index(w) for w in predicted_noisy]

print("Precision:", precision_score(y_true, y_pred, average='macro'))
print("Recall:", recall_score(y_true, y_pred, average='macro'))
print("F1 Score:", f1_score(y_true, y_pred, average='macro'))

# Latency measurement (example)
start_time = time.time()
# Simulated model prediction
_ = [w for w in predicted_noisy]
end_time = time.time()

print("Latency (in seconds):", round(end_time - start_time, 4))

! pip install git+https://github.com/openai/whisper.git -q

import whisper

model = whisper.load_model("base")

model.device

from IPython.display import Audio
Audio("/content/batman.mp3")

from IPython.display import Audio
Audio("/content/_pokiri-vijay-mass-dialog-youtubemp3free.org.mp3")

# load audio and pad/trim it to fit 30 seconds
audio = whisper.load_audio("/content/batman.mp3")
audio = whisper.pad_or_trim(audio)

# make log-Mel spectrogram and move to the same device as the model
mel = whisper.log_mel_spectrogram(audio).to(model.device)

# detect the spoken language
_, probs = model.detect_language(mel)
print(f"Detected language: {max(probs, key=probs.get)}")

# decode the audio
options = whisper.DecodingOptions()
result = whisper.decode(model, mel, options)

# print the recognized text
print(result.text)

# load audio and pad/trim it to fit 30 seconds
audio = whisper.load_audio("/content/_pokiri-vijay-mass-dialog-youtubemp3free.org.mp3")
audio = whisper.pad_or_trim(audio)

# make log-Mel spectrogram and move to the same device as the model
mel = whisper.log_mel_spectrogram(audio).to(model.device)

# detect the spoken language
_, probs = model.detect_language(mel)
print(f"Detected language: {max(probs, key=probs.get)}")

# decode the audio
options = whisper.DecodingOptions()
result = whisper.decode(model, mel, options)

# print the recognized text
print(result.text)

! pip install git+https://github.com/openai/whisper.git -q

import whisper

small_model = whisper.load_model("small")
medium_model = whisper.load_model("medium")
large_model = whisper.load_model("large")

!wget -O audio.mp3 http://www.moviesoundclips.net/movies1/darkknightrises/darkness.mp3

from IPython.display import Audio
Audio("/content/darkness.mp3")

small_result = small_model.transcribe('audio.mp3', fp16=False)
print(small_result["text"])

medium_result = medium_model.transcribe("audio.mp3", fp16=False)
print(medium_result["text"])

large_result = large_model.transcribe("audio.mp3", fp16=False)
print(large_result["text"])